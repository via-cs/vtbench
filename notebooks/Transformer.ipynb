{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ef750aa-9bfb-4cf4-916a-921ed802c5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 samples from ECG5000/ECG5000_TRAIN.ts\n",
      "Loaded 4500 samples from ECG5000/ECG5000_TEST.ts\n",
      "Number of classes: 5\n",
      "y_train unique labels: [0 1 2 3 4]\n",
      "y_test unique labels: [0 1 2 3 4]\n",
      "x_train shape: (500, 139)\n",
      "y_train shape: (500,)\n",
      "x_test shape: (4500, 139)\n",
      "y_test shape: (4500,)\n",
      "x_train_resampled shape: (1460, 139)\n",
      "y_train_resampled shape: (1460,)\n",
      "x_test reduced shape: (675, 139)\n",
      "y_test reduced shape: (675,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vmadhuu\\AppData\\Local\\anaconda3\\envs\\tscproj\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] | Loss: 1.2151, Training accuracy: 54.59%\n",
      "Test Loss after Epoch 1: 0.3198, Test Accuracy after Epoch 1: 96.89%\n",
      "Epoch [2/200] | Loss: 0.9588, Training accuracy: 67.33%\n",
      "Test Loss after Epoch 2: 0.6960, Test Accuracy after Epoch 2: 82.37%\n",
      "Epoch [3/200] | Loss: 0.8006, Training accuracy: 73.22%\n",
      "Test Loss after Epoch 3: 0.2596, Test Accuracy after Epoch 3: 95.56%\n",
      "Epoch [4/200] | Loss: 0.6964, Training accuracy: 75.75%\n",
      "Test Loss after Epoch 4: 0.7722, Test Accuracy after Epoch 4: 71.26%\n",
      "Epoch [5/200] | Loss: 0.6274, Training accuracy: 78.15%\n",
      "Test Loss after Epoch 5: 0.6650, Test Accuracy after Epoch 5: 76.44%\n",
      "Epoch [6/200] | Loss: 0.5728, Training accuracy: 80.27%\n",
      "Test Loss after Epoch 6: 0.7661, Test Accuracy after Epoch 6: 72.44%\n",
      "Epoch [7/200] | Loss: 0.5393, Training accuracy: 81.78%\n",
      "Test Loss after Epoch 7: 0.3505, Test Accuracy after Epoch 7: 91.11%\n",
      "Epoch [8/200] | Loss: 0.5051, Training accuracy: 83.56%\n",
      "Test Loss after Epoch 8: 0.7394, Test Accuracy after Epoch 8: 74.37%\n",
      "Epoch [9/200] | Loss: 0.4778, Training accuracy: 82.19%\n",
      "Test Loss after Epoch 9: 0.1105, Test Accuracy after Epoch 9: 98.67%\n",
      "Epoch [10/200] | Loss: 0.4497, Training accuracy: 85.14%\n",
      "Test Loss after Epoch 10: 0.7862, Test Accuracy after Epoch 10: 69.04%\n",
      "Epoch [11/200] | Loss: 0.4247, Training accuracy: 86.64%\n",
      "Test Loss after Epoch 11: 0.4630, Test Accuracy after Epoch 11: 86.22%\n",
      "Epoch [12/200] | Loss: 0.3997, Training accuracy: 86.16%\n",
      "Test Loss after Epoch 12: 0.8552, Test Accuracy after Epoch 12: 67.26%\n",
      "Epoch [13/200] | Loss: 0.4105, Training accuracy: 86.92%\n",
      "Test Loss after Epoch 13: 0.2403, Test Accuracy after Epoch 13: 95.26%\n",
      "Epoch [14/200] | Loss: 0.3886, Training accuracy: 88.01%\n",
      "Test Loss after Epoch 14: 0.1863, Test Accuracy after Epoch 14: 95.41%\n",
      "Epoch [15/200] | Loss: 0.3757, Training accuracy: 88.49%\n",
      "Test Loss after Epoch 15: 0.1157, Test Accuracy after Epoch 15: 97.93%\n",
      "Epoch [16/200] | Loss: 0.3900, Training accuracy: 87.05%\n",
      "Test Loss after Epoch 16: 0.1833, Test Accuracy after Epoch 16: 96.30%\n",
      "Epoch [17/200] | Loss: 0.3648, Training accuracy: 88.29%\n",
      "Test Loss after Epoch 17: 0.0858, Test Accuracy after Epoch 17: 99.26%\n",
      "Epoch [18/200] | Loss: 0.3678, Training accuracy: 88.15%\n",
      "Test Loss after Epoch 18: 0.2971, Test Accuracy after Epoch 18: 93.63%\n",
      "Epoch [19/200] | Loss: 0.3595, Training accuracy: 87.74%\n",
      "Test Loss after Epoch 19: 0.3627, Test Accuracy after Epoch 19: 87.26%\n",
      "Epoch [20/200] | Loss: 0.3609, Training accuracy: 88.97%\n",
      "Test Loss after Epoch 20: 0.0500, Test Accuracy after Epoch 20: 99.41%\n",
      "Epoch [21/200] | Loss: 0.3466, Training accuracy: 88.36%\n",
      "Test Loss after Epoch 21: 0.4048, Test Accuracy after Epoch 21: 84.89%\n",
      "Epoch [22/200] | Loss: 0.3332, Training accuracy: 89.93%\n",
      "Test Loss after Epoch 22: 0.2385, Test Accuracy after Epoch 22: 95.11%\n",
      "Epoch [23/200] | Loss: 0.3368, Training accuracy: 89.52%\n",
      "Test Loss after Epoch 23: 0.5780, Test Accuracy after Epoch 23: 80.15%\n",
      "Epoch [24/200] | Loss: 0.3495, Training accuracy: 88.70%\n",
      "Test Loss after Epoch 24: 0.1198, Test Accuracy after Epoch 24: 98.07%\n",
      "Epoch [25/200] | Loss: 0.3471, Training accuracy: 88.63%\n",
      "Test Loss after Epoch 25: 0.1371, Test Accuracy after Epoch 25: 97.04%\n",
      "Epoch [26/200] | Loss: 0.3425, Training accuracy: 88.84%\n",
      "Test Loss after Epoch 26: 0.0753, Test Accuracy after Epoch 26: 98.67%\n",
      "Epoch [27/200] | Loss: 0.3297, Training accuracy: 89.93%\n",
      "Test Loss after Epoch 27: 0.3390, Test Accuracy after Epoch 27: 85.93%\n",
      "Epoch [28/200] | Loss: 0.3207, Training accuracy: 89.86%\n",
      "Test Loss after Epoch 28: 0.1847, Test Accuracy after Epoch 28: 95.85%\n",
      "Epoch [29/200] | Loss: 0.3383, Training accuracy: 89.04%\n",
      "Test Loss after Epoch 29: 0.1893, Test Accuracy after Epoch 29: 96.15%\n",
      "Epoch [30/200] | Loss: 0.3382, Training accuracy: 88.90%\n",
      "Test Loss after Epoch 30: 0.1079, Test Accuracy after Epoch 30: 97.93%\n",
      "Epoch [31/200] | Loss: 0.3390, Training accuracy: 89.45%\n",
      "Test Loss after Epoch 31: 0.3322, Test Accuracy after Epoch 31: 93.48%\n",
      "Epoch [32/200] | Loss: 0.3272, Training accuracy: 90.00%\n",
      "Test Loss after Epoch 32: 0.0826, Test Accuracy after Epoch 32: 99.26%\n",
      "Epoch [33/200] | Loss: 0.3213, Training accuracy: 90.68%\n",
      "Test Loss after Epoch 33: 0.1149, Test Accuracy after Epoch 33: 97.63%\n",
      "Epoch [34/200] | Loss: 0.3344, Training accuracy: 89.73%\n",
      "Test Loss after Epoch 34: 0.4966, Test Accuracy after Epoch 34: 86.22%\n",
      "Epoch [35/200] | Loss: 0.3319, Training accuracy: 89.38%\n",
      "Test Loss after Epoch 35: 0.3088, Test Accuracy after Epoch 35: 93.33%\n",
      "Epoch [36/200] | Loss: 0.3186, Training accuracy: 89.86%\n",
      "Test Loss after Epoch 36: 0.0875, Test Accuracy after Epoch 36: 97.78%\n",
      "Epoch [37/200] | Loss: 0.3189, Training accuracy: 90.41%\n",
      "Test Loss after Epoch 37: 0.2359, Test Accuracy after Epoch 37: 95.11%\n",
      "Epoch [38/200] | Loss: 0.3160, Training accuracy: 91.03%\n",
      "Test Loss after Epoch 38: 0.3813, Test Accuracy after Epoch 38: 89.78%\n",
      "Epoch [39/200] | Loss: 0.3065, Training accuracy: 90.62%\n",
      "Test Loss after Epoch 39: 0.0593, Test Accuracy after Epoch 39: 98.96%\n",
      "Epoch [40/200] | Loss: 0.3028, Training accuracy: 91.10%\n",
      "Test Loss after Epoch 40: 0.1420, Test Accuracy after Epoch 40: 97.93%\n",
      "Epoch [41/200] | Loss: 0.3000, Training accuracy: 91.58%\n",
      "Test Loss after Epoch 41: 0.1933, Test Accuracy after Epoch 41: 94.52%\n",
      "Epoch [42/200] | Loss: 0.3033, Training accuracy: 90.75%\n",
      "Test Loss after Epoch 42: 0.7500, Test Accuracy after Epoch 42: 77.63%\n",
      "Epoch [43/200] | Loss: 0.2939, Training accuracy: 91.78%\n",
      "Test Loss after Epoch 43: 0.1362, Test Accuracy after Epoch 43: 97.33%\n",
      "Epoch [44/200] | Loss: 0.3049, Training accuracy: 90.55%\n",
      "Test Loss after Epoch 44: 0.1517, Test Accuracy after Epoch 44: 96.89%\n",
      "Epoch [45/200] | Loss: 0.3115, Training accuracy: 89.52%\n",
      "Test Loss after Epoch 45: 0.1794, Test Accuracy after Epoch 45: 96.15%\n",
      "Epoch [46/200] | Loss: 0.3173, Training accuracy: 90.82%\n",
      "Test Loss after Epoch 46: 0.2799, Test Accuracy after Epoch 46: 94.52%\n",
      "Epoch [47/200] | Loss: 0.3098, Training accuracy: 90.96%\n",
      "Test Loss after Epoch 47: 0.3062, Test Accuracy after Epoch 47: 93.33%\n",
      "Epoch [48/200] | Loss: 0.3068, Training accuracy: 89.45%\n",
      "Test Loss after Epoch 48: 0.1617, Test Accuracy after Epoch 48: 97.33%\n",
      "Epoch [49/200] | Loss: 0.3116, Training accuracy: 89.93%\n",
      "Test Loss after Epoch 49: 0.2788, Test Accuracy after Epoch 49: 92.44%\n",
      "Epoch [50/200] | Loss: 0.3077, Training accuracy: 89.73%\n",
      "Test Loss after Epoch 50: 0.1359, Test Accuracy after Epoch 50: 97.33%\n",
      "Epoch [51/200] | Loss: 0.3085, Training accuracy: 91.03%\n",
      "Test Loss after Epoch 51: 0.0581, Test Accuracy after Epoch 51: 99.56%\n",
      "Epoch [52/200] | Loss: 0.3069, Training accuracy: 90.68%\n",
      "Test Loss after Epoch 52: 0.3328, Test Accuracy after Epoch 52: 93.19%\n",
      "Epoch [53/200] | Loss: 0.3083, Training accuracy: 90.89%\n",
      "Test Loss after Epoch 53: 0.1044, Test Accuracy after Epoch 53: 98.81%\n",
      "Epoch [54/200] | Loss: 0.3061, Training accuracy: 90.96%\n",
      "Test Loss after Epoch 54: 0.2157, Test Accuracy after Epoch 54: 94.81%\n",
      "Epoch [55/200] | Loss: 0.3099, Training accuracy: 90.62%\n",
      "Test Loss after Epoch 55: 0.1079, Test Accuracy after Epoch 55: 98.37%\n",
      "Epoch [56/200] | Loss: 0.3431, Training accuracy: 87.60%\n",
      "Test Loss after Epoch 56: 0.1599, Test Accuracy after Epoch 56: 96.15%\n",
      "Epoch [57/200] | Loss: 0.3374, Training accuracy: 88.84%\n",
      "Test Loss after Epoch 57: 0.1194, Test Accuracy after Epoch 57: 97.78%\n",
      "Epoch [58/200] | Loss: 0.3038, Training accuracy: 91.85%\n",
      "Test Loss after Epoch 58: 0.3126, Test Accuracy after Epoch 58: 92.30%\n",
      "Epoch [59/200] | Loss: 0.3058, Training accuracy: 90.55%\n",
      "Test Loss after Epoch 59: 0.0669, Test Accuracy after Epoch 59: 99.11%\n",
      "Epoch [60/200] | Loss: 0.2973, Training accuracy: 91.23%\n",
      "Test Loss after Epoch 60: 0.4006, Test Accuracy after Epoch 60: 89.63%\n",
      "Epoch [61/200] | Loss: 0.3020, Training accuracy: 90.75%\n",
      "Test Loss after Epoch 61: 0.1517, Test Accuracy after Epoch 61: 96.74%\n",
      "Epoch [62/200] | Loss: 0.3116, Training accuracy: 90.34%\n",
      "Test Loss after Epoch 62: 0.2435, Test Accuracy after Epoch 62: 94.81%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 331\u001b[0m\n\u001b[0;32m    327\u001b[0m data, labels \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    329\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 331\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[0;32m    332\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m    334\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tscproj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tscproj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[32], line 294\u001b[0m, in \u001b[0;36mSimplifiedTransformerModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    292\u001b[0m x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_encoding\n\u001b[0;32m    293\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Transformer expects input of shape (seq_len, batch, dim_model)\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder(x)\n\u001b[0;32m    295\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    296\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tscproj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tscproj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tscproj\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:415\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    412\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 415\u001b[0m     output \u001b[38;5;241m=\u001b[39m mod(output, src_mask\u001b[38;5;241m=\u001b[39mmask, is_causal\u001b[38;5;241m=\u001b[39mis_causal, src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask_for_layers)\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    418\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tscproj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tscproj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tscproj\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:749\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    747\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 749\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[38;5;241m=\u001b[39mis_causal))\n\u001b[0;32m    750\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tscproj\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:757\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[0;32m    756\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 757\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(x, x, x,\n\u001b[0;32m    758\u001b[0m                        attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[0;32m    759\u001b[0m                        key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[0;32m    760\u001b[0m                        need_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, is_causal\u001b[38;5;241m=\u001b[39mis_causal)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tscproj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tscproj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tscproj\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1266\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1252\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1253\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m   1254\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1263\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1264\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m   1265\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1266\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1267\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m   1268\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   1269\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_v, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_zero_attn,\n\u001b[0;32m   1270\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m   1271\u001b[0m         training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[0;32m   1272\u001b[0m         key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[0;32m   1273\u001b[0m         need_weights\u001b[38;5;241m=\u001b[39mneed_weights,\n\u001b[0;32m   1274\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[0;32m   1275\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1276\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m   1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tscproj\\Lib\\site-packages\\torch\\nn\\functional.py:5364\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   5362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[0;32m   5363\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m in_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is False but in_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 5364\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n\u001b[0;32m   5365\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5366\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m q_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is True but q_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tscproj\\Lib\\site-packages\\torch\\nn\\functional.py:4884\u001b[0m, in \u001b[0;36m_in_projection_packed\u001b[1;34m(q, k, v, w, b)\u001b[0m\n\u001b[0;32m   4881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mis\u001b[39;00m v:\n\u001b[0;32m   4882\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m q \u001b[38;5;129;01mis\u001b[39;00m k:\n\u001b[0;32m   4883\u001b[0m         \u001b[38;5;66;03m# self-attention\u001b[39;00m\n\u001b[1;32m-> 4884\u001b[0m         proj \u001b[38;5;241m=\u001b[39m linear(q, w, b)\n\u001b[0;32m   4885\u001b[0m         \u001b[38;5;66;03m# reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\u001b[39;00m\n\u001b[0;32m   4886\u001b[0m         proj \u001b[38;5;241m=\u001b[39m proj\u001b[38;5;241m.\u001b[39munflatten(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m3\u001b[39m, E))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### Transformer doing Multiclass Classification using ECG5000 datasset\n",
    "\n",
    "# ### Binary Classification\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# import matplotlib.pyplot as plt\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# # Device configuration\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# # Read UCR Dataset\n",
    "# def read_ucr(filename):\n",
    "#     data = []\n",
    "#     labels = []\n",
    "    \n",
    "#     with open(filename, 'r') as file:\n",
    "#         for line in file:\n",
    "#             parts = line.strip().split(',')\n",
    "#             if len(parts) < 2:  # Ensure there's at least one feature and one label\n",
    "#                 continue\n",
    "#             features = [float(f) for f in parts[:-1]]\n",
    "#             label = int(parts[-1].split(':')[-1])  # Handle label after the colon\n",
    "#             data.append(features)\n",
    "#             labels.append(label)\n",
    "    \n",
    "#     print(f\"Loaded {len(data)} samples from {filename}\")\n",
    "#     return np.array(data), np.array(labels)\n",
    "\n",
    "# train_file = 'ECG/ECG_TRAIN.ts'\n",
    "# test_file = 'ECG/ECG_TEST.ts'\n",
    "\n",
    "# # Load dataset\n",
    "# x_train, y_train = read_ucr(train_file)\n",
    "# x_test, y_test = read_ucr(test_file)\n",
    "\n",
    "# # Normalize labels to be within range [0, num_classes-1]\n",
    "# unique_labels = np.unique(y_train)\n",
    "# label_map = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "# y_train = np.array([label_map[label] for label in y_train])\n",
    "# y_test = np.array([label_map[label] for label in y_test])\n",
    "\n",
    "# nb_classes = len(unique_labels)\n",
    "\n",
    "# # Verify labels are within range\n",
    "# print(f\"Number of classes: {nb_classes}\")\n",
    "# print(f\"y_train unique labels: {np.unique(y_train)}\")\n",
    "# print(f\"y_test unique labels: {np.unique(y_test)}\")\n",
    "\n",
    "# # Ensure labels are within the expected range [0, num_classes-1]\n",
    "# assert y_train.min() >= 0 and y_train.max() < nb_classes, \"Train labels are out of range\"\n",
    "# assert y_test.min() >= 0 and y_test.max() < nb_classes, \"Test labels are out of range\"\n",
    "\n",
    "# # Print shapes to ensure they match\n",
    "# print(f\"x_train shape: {x_train.shape}\")\n",
    "# print(f\"y_train shape: {y_train.shape}\")\n",
    "# print(f\"x_test shape: {x_test.shape}\")\n",
    "# print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# # Apply SMOTE to the training data\n",
    "# smote = SMOTE()\n",
    "# x_train_resampled, y_train_resampled = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "# # Convert to PyTorch tensors\n",
    "# X_train = torch.tensor(x_train_resampled, dtype=torch.float32).unsqueeze(-1)  # Add feature dimension\n",
    "# X_test = torch.tensor(x_test, dtype=torch.float32).unsqueeze(-1)\n",
    "# y_train = torch.tensor(y_train_resampled, dtype=torch.long)\n",
    "# y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# # Create Data Loaders\n",
    "# train_dataset = TensorDataset(X_train, y_train)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# test_dataset = TensorDataset(X_test, y_test)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# # Plot class distribution\n",
    "# plt.hist(y_train.numpy(), bins=nb_classes, edgecolor='k')\n",
    "# plt.title('Class Distribution in Training Set')\n",
    "# plt.xlabel('Class')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.xticks(ticks=np.arange(nb_classes))\n",
    "# plt.show()\n",
    "\n",
    "# # Define the Transformer model with Batch Normalization and increased complexity\n",
    "# class TransformerModel(nn.Module):\n",
    "#     def __init__(self, input_dim, num_classes, dim_model=128, num_heads=8, num_layers=4, dropout=0.3):\n",
    "#         super(TransformerModel, self).__init__()\n",
    "#         self.embedding = nn.Linear(input_dim, dim_model)\n",
    "#         self.position_encoding = nn.Parameter(torch.zeros(1, 100, dim_model))  # Assuming sequence length of 100\n",
    "#         encoder_layer = nn.TransformerEncoderLayer(d_model=dim_model, nhead=num_heads, dropout=dropout)\n",
    "#         self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "#         self.bn = nn.BatchNorm1d(dim_model)\n",
    "#         self.fc = nn.Linear(dim_model, num_classes)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.embedding(x)\n",
    "#         x += self.position_encoding[:, :x.size(1), :]\n",
    "#         x = x.permute(1, 0, 2)  # Transformer expects input of shape (seq_len, batch, dim_model)\n",
    "#         x = self.transformer_encoder(x)\n",
    "#         x = x.mean(dim=0)\n",
    "#         x = self.bn(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "# # Initialize the Transformer model\n",
    "# input_dim = 1  # Since each time step has a single feature\n",
    "# dim_model = 128\n",
    "# num_heads = 8\n",
    "# num_layers = 4\n",
    "# dropout = 0.3\n",
    "# num_classes = nb_classes\n",
    "# model = TransformerModel(input_dim, num_classes, dim_model, num_heads, num_layers, dropout).to(device)\n",
    "\n",
    "# # Training setup\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.01)\n",
    "# scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.005, steps_per_epoch=len(train_loader), epochs=num_epochs)\n",
    "\n",
    "# # Training loop with early stopping\n",
    "# num_epochs = 200\n",
    "# best_test_accuracy = 0\n",
    "# patience = 10\n",
    "# trigger_times = 0\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for data, labels in train_loader:\n",
    "#         data, labels = data.to(device), labels.to(device)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         outputs = model(data)\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         loss.backward()\n",
    "#         nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     epoch_loss = running_loss / len(train_loader)\n",
    "#     epoch_accuracy = 100 * correct / total\n",
    "#     print(f'Epoch [{epoch+1}/{num_epochs}] | Loss: {epoch_loss:.4f}, Training accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "#     # Validation\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for data, labels in test_loader:\n",
    "#             data, labels = data.to(device), labels.to(device)\n",
    "#             outputs = model(data)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "    \n",
    "#     test_accuracy = 100 * correct / total\n",
    "#     print(f'Test Accuracy after Epoch {epoch+1}: {test_accuracy:.2f}%')\n",
    "#     scheduler.step()\n",
    "\n",
    "#     # Check for early stopping\n",
    "#     if test_accuracy > best_test_accuracy:\n",
    "#         best_test_accuracy = test_accuracy\n",
    "#     #     trigger_times = 0\n",
    "#     # else:\n",
    "#     #     trigger_times += 1\n",
    "#     #     if trigger_times >= patience:\n",
    "#     #         print(f'Early stopping at epoch {epoch+1}')\n",
    "#     #         break\n",
    "\n",
    "# print(f'Best Test Accuracy: {best_test_accuracy:.2f}%')\n",
    "\n",
    "\n",
    "### Multiclass Classification\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Read UCR Dataset\n",
    "def read_ucr(filename):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(',')\n",
    "            if len(parts) < 2:  # Ensure there's at least one feature and one label\n",
    "                continue\n",
    "            features = [float(f) for f in parts[:-1]]\n",
    "            label = int(parts[-1].split(':')[-1])  # Handle label after the colon\n",
    "            data.append(features)\n",
    "            labels.append(label)\n",
    "    \n",
    "    print(f\"Loaded {len(data)} samples from {filename}\")\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "train_file = 'ECG5000/ECG5000_TRAIN.ts'\n",
    "test_file = 'ECG5000/ECG5000_TEST.ts'\n",
    "\n",
    "# Load dataset\n",
    "x_train, y_train = read_ucr(train_file)\n",
    "x_test, y_test = read_ucr(test_file)\n",
    "\n",
    "# Normalize labels to be within range [0, num_classes-1]\n",
    "unique_labels = np.unique(y_train)\n",
    "label_map = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "y_train = np.array([label_map[label] for label in y_train])\n",
    "y_test = np.array([label_map[label] for label in y_test])\n",
    "\n",
    "nb_classes = len(unique_labels)\n",
    "\n",
    "# Verify labels are within range\n",
    "print(f\"Number of classes: {nb_classes}\")\n",
    "print(f\"y_train unique labels: {np.unique(y_train)}\")\n",
    "print(f\"y_test unique labels: {np.unique(y_test)}\")\n",
    "\n",
    "# Ensure labels are within the expected range [0, num_classes-1]\n",
    "assert y_train.min() >= 0 and y_train.max() < nb_classes, \"Train labels are out of range\"\n",
    "assert y_test.min() >= 0 and y_test.max() < nb_classes, \"Test labels are out of range\"\n",
    "\n",
    "# Print shapes to ensure they match\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(k_neighbors=1)\n",
    "x_train_resampled, y_train_resampled = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "# Limit test data to 675 samples\n",
    "x_test = x_test[:675]\n",
    "y_test = y_test[:675]\n",
    "\n",
    "# Print new shapes to ensure they match\n",
    "print(f\"x_train_resampled shape: {x_train_resampled.shape}\")\n",
    "print(f\"y_train_resampled shape: {y_train_resampled.shape}\")\n",
    "print(f\"x_test reduced shape: {x_test.shape}\")\n",
    "print(f\"y_test reduced shape: {y_test.shape}\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(x_train_resampled, dtype=torch.float32).unsqueeze(-1)  # Add feature dimension\n",
    "X_test = torch.tensor(x_test, dtype=torch.float32).unsqueeze(-1)\n",
    "y_train = torch.tensor(y_train_resampled, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create Data Loaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Plot class distribution\n",
    "plt.hist(y_train.numpy(), bins=nb_classes, edgecolor='k')\n",
    "plt.title('Class Distribution in Training Set')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(ticks=np.arange(nb_classes))\n",
    "plt.show()\n",
    "\n",
    "class SimplifiedTransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, seq_len, dim_model=32, num_heads=2, num_layers=2, dropout=0.2):\n",
    "        super(SimplifiedTransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, dim_model)\n",
    "        self.position_encoding = nn.Parameter(torch.zeros(seq_len, dim_model))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim_model, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(dim_model, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bn = nn.BatchNorm1d(dim_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x = self.embedding(x)\n",
    "        position_encoding = self.position_encoding.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        x += position_encoding\n",
    "        x = x.permute(1, 0, 2)  # Transformer expects input of shape (seq_len, batch, dim_model)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=0)\n",
    "        x = self.bn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the simpler Transformer model\n",
    "input_dim = 1  # Since each time step has a single feature\n",
    "seq_len = X_train.shape[1]\n",
    "dim_model = 32\n",
    "num_heads = 2\n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "num_classes = nb_classes\n",
    "model = SimplifiedTransformerModel(input_dim, num_classes, seq_len, dim_model, num_heads, num_layers, dropout).to(device)\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.005, steps_per_epoch=len(train_loader), epochs=num_epochs)\n",
    "\n",
    "# Training loop without early stopping\n",
    "num_epochs = 200\n",
    "\n",
    "best_test_accuracy = 0  # Variable to track the best test accuracy\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, labels in train_loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] | Loss: {epoch_loss:.4f}, Training accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.inference_mode():\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)  # Calculate test loss\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            logits = torch.softmax(outputs, dim=1)  # Apply softmax to get probabilities\n",
    "            _, predicted = torch.max(logits, 1)  # Get the class with the highest probability\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f'Test Loss after Epoch {epoch+1}: {test_loss:.4f}, Test Accuracy after Epoch {epoch+1}: {test_accuracy:.2f}%')\n",
    "\n",
    "    # Update the best test accuracy if the current test accuracy is better\n",
    "    if test_accuracy > best_test_accuracy:\n",
    "        best_test_accuracy = test_accuracy\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "print(f'Best Test Accuracy: {best_test_accuracy:.2f}%')\n",
    "\n",
    "# Plot the training and testing loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(test_losses, label='Testing Loss')\n",
    "plt.title('Training and Testing Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99b73d3-324d-4563-8259-a3acdda94fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vmadhu",
   "language": "python",
   "name": "tscproj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
